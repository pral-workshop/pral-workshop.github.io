import{t as _,a as R,n as Ae,d as j}from"../chunks/DeQyGRAF.js";import{i as ce}from"../chunks/BUKhciG0.js";import{a7 as Pe,a3 as Q,Y as S,$ as Ce,_ as Re,a2 as de,Q as Me,g as J,J as ue,ak as we,al as re,ab as K,Z as G,ae as Ie,am as pe,V as me,an as _e,ai as he,B as We,K as xe,q as ne,at as ee,y as ie,au as Se,av as Te,aw as ze,ah as De,ax as Ne,X as Ee,ay as Fe,az as Ye,aa as He,A as Oe,aA as $e,aB as Be,p as ge,t as Z,j as fe,k as y,m as L,s as p,aC as D,aD as je,i as oe}from"../chunks/DX9BjxLj.js";import{i as Ge,c as qe,d as Je,n as Ke,a as Ue,s as V,b as Ze}from"../chunks/BvO99789.js";import{l as Xe,p as Y,r as Ve}from"../chunks/BIriWe9Q.js";function Qe(e,a){if(a){const t=document.body;e.autofocus=!0,Pe(()=>{document.activeElement===t&&e.focus()})}}function ea(e,a){return a}function aa(e,a,t,r){for(var n=[],i=a.length,s=0;s<i;s++)ze(a[s].e,n,!0);var h=i>0&&n.length===0&&t!==null;if(h){var d=t.parentNode;De(d),d.append(t),r.clear(),N(e,a[0].prev,a[i-1].next)}Ne(n,()=>{for(var k=0;k<i;k++){var o=a[k];h||(r.delete(o.k),N(e,o.prev,o.next)),Ee(o.e,!h)}})}function ta(e,a,t,r,n,i=null){var s=e,h={items:new Map,first:null};{var d=e;s=S?Q(Ce(d)):d.appendChild(Re())}S&&de();var k=null,o=!1,P=ue(()=>{var l=t();return We(l)?l:l==null?[]:he(l)});Me(()=>{var l=J(P),v=l.length;if(o&&v===0)return;o=v===0;let g=!1;if(S){var x=s.data===we;x!==(v===0)&&(s=re(),Q(s),K(!1),g=!0)}if(S){for(var A=null,f,c=0;c<v;c++){if(G.nodeType===8&&G.data===Ie){s=G,g=!0,K(!1);break}var b=l[c],C=r(b,c);f=Le(G,h,A,null,b,C,c,n,a,t),h.items.set(C,f),A=f}v>0&&Q(re())}S||ra(l,h,s,n,a,r,t),i!==null&&(v===0?k?pe(k):k=me(()=>i(s)):k!==null&&_e(k,()=>{k=null})),g&&K(!0),J(P)}),S&&(s=G)}function ra(e,a,t,r,n,i,s){var h=e.length,d=a.items,k=a.first,o=k,P,l=null,v=[],g=[],x,A,f,c;for(c=0;c<h;c+=1){if(x=e[c],A=i(x,c),f=d.get(A),f===void 0){var b=o?o.e.nodes_start:t;l=Le(b,a,l,l===null?a.first:l.next,x,A,c,r,n,s),d.set(A,l),v=[],g=[],o=l.next;continue}if(na(f,x,c),f.e.f&ee&&pe(f.e),f!==o){if(P!==void 0&&P.has(f)){if(v.length<g.length){var C=g[0],M;l=C.prev;var w=v[0],E=v[v.length-1];for(M=0;M<v.length;M+=1)se(v[M],C,t);for(M=0;M<g.length;M+=1)P.delete(g[M]);N(a,w.prev,E.next),N(a,l,w),N(a,E,C),o=C,l=E,c-=1,v=[],g=[]}else P.delete(f),se(f,o,t),N(a,f.prev,f.next),N(a,f,l===null?a.first:l.next),N(a,l,f),l=f;continue}for(v=[],g=[];o!==null&&o.k!==A;)o.e.f&ee||(P??(P=new Set)).add(o),g.push(o),o=o.next;if(o===null)continue;f=o}v.push(f),l=f,o=f.next}if(o!==null||P!==void 0){for(var u=P===void 0?[]:he(P);o!==null;)o.e.f&ee||u.push(o),o=o.next;var W=u.length;if(W>0){var m=h===0?t:null;aa(a,u,m,d)}}ie.first=a.first&&a.first.e,ie.last=l&&l.e}function na(e,a,t,r){Te(e.v,a),e.i=t}function Le(e,a,t,r,n,i,s,h,d,k){var o=(d&Fe)!==0,P=(d&Ye)===0,l=o?P?xe(n):ne(n):n,v=d&Se?ne(s):s,g={i:v,v:l,k:i,a:null,e:null,prev:t,next:r};try{return g.e=me(()=>h(e,l,v,k),S),g.e.prev=t&&t.e,g.e.next=r&&r.e,t===null?a.first=g:(t.next=g,t.e.next=g.e),r!==null&&(r.prev=g,r.e.prev=g.e),g}finally{}}function se(e,a,t){for(var r=e.next?e.next.e.nodes_start:t,n=a?a.e.nodes_start:t,i=e.e.nodes_start;i!==r;){var s=He(i);n.before(i),i=s}}function N(e,a,t){a===null?e.first=t:(a.next=t,a.e.next=t&&t.e),t!==null&&(t.prev=a,t.e.prev=a&&a.e)}function ve(e,a,t,r,n){var h;S&&de();var i=(h=a.$$slots)==null?void 0:h[t],s=!1;i===!0&&(i=a.children,s=!0),i===void 0||i(e,s?()=>r:r)}function be(e){var a,t,r="";if(typeof e=="string"||typeof e=="number")r+=e;else if(typeof e=="object")if(Array.isArray(e)){var n=e.length;for(a=0;a<n;a++)e[a]&&(t=be(e[a]))&&(r&&(r+=" "),r+=t)}else for(t in e)e[t]&&(r&&(r+=" "),r+=t);return r}function ia(){for(var e,a,t=0,r="",n=arguments.length;t<n;t++)(e=arguments[t])&&(a=be(e))&&(r&&(r+=" "),r+=a);return r}function oa(e){return typeof e=="object"?ia(e):e??""}function sa(e,a){a?e.hasAttribute("selected")||e.setAttribute("selected",""):e.removeAttribute("selected")}function U(e,a,t,r){var n=e.__attributes??(e.__attributes={});S&&(n[a]=e.getAttribute(a),a==="src"||a==="srcset"||a==="href"&&e.nodeName==="LINK")||n[a]!==(n[a]=t)&&(a==="style"&&"__styles"in e&&(e.__styles={}),a==="loading"&&(e[$e]=t),t==null?e.removeAttribute(a):typeof t!="string"&&ye(e).includes(a)?e[a]=t:e.setAttribute(a,t))}function la(e,a,t,r,n=!1,i=!1,s=!1){let h=S&&i;h&&K(!1);var d=a||{},k=e.tagName==="OPTION";for(var o in a)o in t||(t[o]=null);t.class&&(t.class=oa(t.class));var P=ye(e),l=e.__attributes??(e.__attributes={});for(const c in t){let b=t[c];if(k&&c==="value"&&b==null){e.value=e.__value="",d[c]=b;continue}var v=d[c];if(b!==v){d[c]=b;var g=c[0]+c[1];if(g!=="$$"){if(g==="on"){const C={},M="$$"+c;let w=c.slice(2);var x=Ue(w);if(Ge(w)&&(w=w.slice(0,-7),C.capture=!0),!x&&v){if(b!=null)continue;e.removeEventListener(w,d[M],C),d[M]=null}if(b!=null)if(x)e[`__${w}`]=b,Je([w]);else{let E=function(u){d[c].call(this,u)};d[M]=qe(w,e,E,C)}else x&&(e[`__${w}`]=void 0)}else if(c==="style"&&b!=null)e.style.cssText=b+"";else if(c==="autofocus")Qe(e,!!b);else if(!i&&(c==="__value"||c==="value"&&b!=null))e.value=e.__value=b;else if(c==="selected"&&k)sa(e,b);else{var A=c;n||(A=Ke(A));var f=A==="defaultValue"||A==="defaultChecked";if(b==null&&!i&&!f)if(l[c]=null,A==="value"||A==="checked"){let C=e;if(A==="value"){let M=C.defaultValue;C.removeAttribute(A),C.defaultValue=M}else{let M=C.defaultChecked;C.removeAttribute(A),C.defaultChecked=M}}else e.removeAttribute(c);else f||P.includes(A)&&(i||typeof b!="string")?e[A]=b:typeof b!="function"&&U(e,A,b)}c==="style"&&"__styles"in e&&(e.__styles={})}}}return h&&K(!0),d}var le=new Map;function ye(e){var a=le.get(e.nodeName);if(a)return a;le.set(e.nodeName,a=[]);for(var t,r=e,n=Element.prototype;n!==r;){t=Be(r);for(var i in t)t[i].set&&a.push(i);r=Oe(r)}return a}function ca(e,a,t){var r=e.__className,n=da(a);S&&e.className===n?e.__className=n:(r!==n||S&&e.className!==n)&&(a==null?e.removeAttribute("class"):e.className=n,e.__className=n)}function da(e,a){return(e??"")+""}var ua=_('<div class="flex justify-center font-serif text-lg"><div><!></div></div>');function z(e,a){const t=Xe(a,["children","$$slots","$$events","$$legacy"]);ge(a,!1);let r=Y(a,"size",8,"max-w-2xl"),n=Y(a,"padding",8,"pb-12");ce();var i=ua(),s=y(i),h=y(s);ve(h,a,"default",{}),L(s),L(i),Z(()=>ca(s,`mx-8 ${r()??""} w-full ${n()??""} ${t.class??""}`)),R(e,i),fe()}var pa=Ae('<svg><path fill="currentColor" d="M12 2A10 10 0 0 0 2 12c0 4.42 2.87 8.17 6.84 9.5c.5.08.66-.23.66-.5v-1.69c-2.77.6-3.36-1.34-3.36-1.34c-.46-1.16-1.11-1.47-1.11-1.47c-.91-.62.07-.6.07-.6c1 .07 1.53 1.03 1.53 1.03c.87 1.52 2.34 1.07 2.91.83c.09-.65.35-1.09.63-1.34c-2.22-.25-4.55-1.11-4.55-4.92c0-1.11.38-2 1.03-2.71c-.1-.25-.45-1.29.1-2.64c0 0 .84-.27 2.75 1.02c.79-.22 1.65-.33 2.5-.33s1.71.11 2.5.33c1.91-1.29 2.75-1.02 2.75-1.02c.55 1.35.2 2.39.1 2.64c.65.71 1.03 1.6 1.03 2.71c0 3.82-2.34 4.66-4.57 4.91c.36.31.69.92.69 1.85V21c0 .27.16.59.67.5C19.14 20.16 22 16.42 22 12A10 10 0 0 0 12 2"></path></svg>');function ma(e,a){const t=Ve(a,["$$slots","$$events","$$legacy"]);var r=pa();let n;Z(()=>n=la(r,n,{viewBox:"0 0 24 24",width:"1.2em",height:"1.2em",...t},void 0,!0)),R(e,r)}var ha=_('<a target="_blank" rel="noreferrer" class="inline-block"><div class="flex cursor-pointer flex-col items-center justify-center px-4 py-4"><div class="h-32 w-32 rounded-full border border-gray-300 bg-cover bg-center"></div> <div class="mt-2 mb-2 max-w-32 border-gray-300 text-center text-xl underline decoration-gray-300 underline-offset-6 transition hover:decoration-gray-500"> </div> <div class="max-w-32 text-center text-base text-gray-600"> </div></div></a>');function I(e,a){let t=Y(a,"name",8,"John Doe"),r=Y(a,"affiliation",8,"Software Engineer"),n=Y(a,"image",8,""),i=Y(a,"link",8,"");var s=ha(),h=y(s),d=y(h),k=p(d,2),o=y(k,!0);L(k);var P=p(k,2),l=y(P,!0);L(P),L(h),L(s),Z(()=>{U(s,"href",i()),U(d,"style",`background-image: url(${n()??""})`),V(o,t()),V(l,r())}),R(e,s)}var ga=_('<a><div class="my-2 cursor-pointer font-mono text-xs text-gray-600 uppercase underline underline-offset-4 transition hover:bg-gray-600 hover:text-white"><!></div></a>');function q(e,a){let t=Y(a,"href",8,"");var r=ga(),n=y(r),i=y(n);ve(i,a,"default",{}),L(n),L(r),Z(()=>U(r,"href",t())),R(e,r)}var fa=_('<p><a href="https://openreview.net/group?id=ICML.cc/2025/Workshop/PRAL" rel="nofollow"><u>Open Review</u></a></p> <p>We invite the submission of research papers and position papers on the topic of programmatic representations for agent learning. This workshop aims to explore the use of program-like structures to represent policies, reward functions, tasks, and environment models.</p> <p>Topics of interest include, but are not limited to:</p> <ul><li><strong>Programs as Policies:</strong> Representing decision-making logic through programmatic policies in Python or domain-specific languages.</li> <li><strong>Programs as Reward Functions:</strong> Synthesizing reward function codes for agent learning.</li> <li><strong>Programs as Skill Libraries:</strong> Representing acquired skills as programs, allowing for reusing and composing skills.</li> <li><strong>Programmatically Generating Tasks:</strong> Producing codes that describe diverse task variants.</li> <li><strong>Programs as Environment Models:</strong> Inferring executable codes to simulate environment dynamics.</li></ul> <p><strong>Submission Types:</strong></p> <ul><li><strong>Full Papers:</strong> Up to 9 pages in ICML or NeurIPS format, with potentially large-scale experiments.</li> <li><strong>Short Papers:</strong> 2-4 pages in ICML or NeurIPS format, with proof-of-concept demonstrations (demos, code, blog posts).</li></ul> <p><strong>Important Dates:</strong></p> <ul><li>Submission Deadline: <del>May 24, 2025, AoE</del> May 30, 2025, AoE</li> <li>Author Notification: <del>June 7, 2025, AoE</del> June 13, 2025, AoE</li> <li>Camera Ready Deadline: July 7, 2025, AoE</li> <li>Workshop Date: July 18, 2025</li></ul> <p>Accepted papers will be presented during poster sessions, with exceptional submissions selected for spotlight oral presentations.</p> <p>All accepted papers will be made publicly available as non-archival reports, allowing for future submissions to archival conferences or journals.</p> <p>Please submit your papers to the <a href="https://openreview.net/group?id=ICML.cc/2025/Workshop/PRAL" rel="nofollow"><u>Open Review</u></a> site.</p> <h1 class="py-4 text-2xl font-bold" id="camera-ready">Camera Ready Instructions</h1> <p>Please incorporate reviewers’ feedbacks and prepare for your camera-ready submission. Please submit your camera-ready version on OpenReview. Your camera-ready submission should be de-anonymized, and include at most 9 pages for full papers, and 2-4 pages for short papers, excluding the references and appendices. The paper can be in ICML or NeurIPS formats, with footnote “ICML 2025 Workshop on Programmatic Representations for Agent Learning”.</p> <p>Camera-Ready LaTeX Templates:</p> <ul><li><a href="/tex/icml2025_pral.sty"><u>ICML Format</u></a></li> <li><a href="/tex/icml2025_pral_neurips.sty"><u>NeurIPS Format</u></a></li></ul> <p>The camera-ready deadline is July 7, 2025, Anywhere on Earth (AoE).</p>',1);function La(e){var a=fa();D(30),R(e,a)}var va=_('<p>This workshop explores using programmatic representations (e.g., code, symbolic programs, rules) to enhance agent learning and address key challenges in creating autonomous agents.  By leveraging structured representations, we aim to improve interpretability, generalization, efficiency, and safety in agent systems, moving beyond the limitations of “black box” deep learning models.  The workshop brings together researchers in sequential decision-making and program synthesis/code generation to discuss using <strong>programs as policies</strong> (e.g., <a href="https://arxiv.org/abs/2108.13643"><span style="color:blue">LEAPS</span></a>, <a href="https://arxiv.org/abs/2209.07753"><span style="color:blue">Code as Policies</span></a>, <a href="https://arxiv.org/abs/2301.12950"><span style="color:blue">HPRL</span></a>, <a href="https://arxiv.org/abs/2310.13065"><span style="color:blue">RoboTool</span></a>, <a href="https://arxiv.org/abs/2410.12166"><span style="color:blue">Carvalho et al. 2024</span></a>), <strong>reward functions</strong> (e.g., <a href="https://arxiv.org/abs/2310.12931"><span style="color:blue">Eureka</span></a>, <a href="https://arxiv.org/abs/2306.08647"><span style="color:blue">Language2Reward</span></a>, <a href="https://arxiv.org/abs/2309.11489"><span style="color:blue">Text2Reward</span></a>), <strong>skill libraries</strong> (e.g., <a href="https://arxiv.org/abs/2305.16291"><span style="color:blue">Voyager</span></a>), <strong>task generators</strong> (e.g., <a href="https://arxiv.org/abs/2310.01361"><span style="color:blue">GenSim</span></a>), or <strong>environment models</strong> (e.g., <a href="https://arxiv.org/abs/2402.12275"><span style="color:blue">WorldCoder</span></a>, <a href="https://arxiv.org/abs/2405.15383"><span style="color:blue">Code World Models</span></a>), ultimately driving progress toward robust, understandable, and adaptable autonomous agents across diverse applications.</p>');function ba(e){var a=va();R(e,a)}const ya=JSON.parse(`[{"id":"uRV6vlXiOQ","forum":"uRV6vlXiOQ","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission32/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission32/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission32/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission32/Authors"],"content":{"title":{"value":"Optimizing Agentic Architectures for Cybersecurity Tasks with Trace"},"authors":{"value":["Anish Chaudhuri","Prerit Choudhary","Max Piasevoli","Shannon Xiao","Allen Nie"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission32/Authors"]},"authorids":{"value":["~Anish_Chaudhuri1","~Prerit_Choudhary1","~Max_Piasevoli1","~Shannon_Xiao1","~Allen_Nie1"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission32/Authors"]},"keywords":{"value":["Cybersecurity","Large Language Model Optimizer","CTF Tasks","Agent Learning"]},"TLDR":{"value":"We leverage Trace-based agentic architectures—combined with both actor-only & actor-critic methods—to iteratively optimize LLM reasoning for cybersecurity CTFs, achieving 25% success on CyBench."},"track":{"value":"Short Paper (up to 4 pages)"},"abstract":{"value":"In this work, we introduce a novel agentic workflow that leverages Trace, a computational graph-based framework that analyzes execution traces via Directed Acyclic Graphs (DAGs) to systematically refine LLM reasoning in cybersecurity tasks. By structuring execution as a graph traversal problem, our approach enhances the model’s ability to iteratively generate, analyze, and optimize its code-based solutions, improving both reasoning depth and task success rates. We implement CyTrace on a subset of CTF tasks derived from the CyBench benchmark, covering domains such as cryptography, reverse engineering, steganography, and exploit development. \\nOur proposed approach solves 10 tasks, achieving a $25\\\\%$ solved rate, compared to $17.5\\\\%$ from the base model alone, and outperforming o3-mini $(22.5\\\\%)$."},"pdf":{"value":"/pdf/d3661a2c762d3dc938355c7b1dd09be44304b6e5.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"supplementary_material":{"value":"/attachment/a146a47457e995dc2ddc6ddc96db3babc0ee4151.zip"},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Anish_Chaudhuri1"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025optimizing,\\ntitle={Optimizing Agentic Architectures for Cybersecurity Tasks with Trace},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=uRV6vlXiOQ}\\n}"},"paperhash":{"value":"chaudhuri|optimizing_agentic_architectures_for_cybersecurity_tasks_with_trace","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission32/Authors"]}},"number":32,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1748692737992,"cdate":1748692737992,"tmdate":1749884845063,"mdate":1749884845063,"pdate":1749884845032,"version":2,"details":{"writable":true,"replyCount":4,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Short Paper (up to 4 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"nqWsNxbkDV","forum":"nqWsNxbkDV","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission31/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission31/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission31/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission31/Authors"],"content":{"title":{"value":"Leveraging Learned Programmatic Facts for Enhanced LLM Agent Planning and World Modeling"},"authors":{"value":["Samuel Holt","Max Ruiz Luyten","Thomas Pouplin","Mihaela van der Schaar"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission31/Authors"]},"authorids":{"value":["~Samuel_Holt1","~Max_Ruiz_Luyten1","~Thomas_Pouplin1","~Mihaela_van_der_Schaar2"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission31/Authors"]},"keywords":{"value":["LLM Agents","Programmatic Representations","In-Context Learning","Learned Programmatic Facts","World Modeling","Planning","Lookahead Search","Sequential Decision-Making","Interpretability"]},"TLDR":{"value":"An LLM agent learns programmatic textual facts as structured in-context knowledge to enhance its planning and internal world model via lookahead search."},"track":{"value":"Long Paper (up to 9 pages)"},"abstract":{"value":"Large Language Models (LLMs) are increasingly capable but often require significant guidance or extensive interaction history to perform effectively in complex, interactive environments, partly due to the \\"black box\\" nature of their internal reasoning. Existing methods may struggle with adapting to new information or efficiently utilizing past experiences for multi-step reasoning without fine-tuning. We introduce a novel LLM agent framework that enhances planning capabilities through in-context learning, facilitated by augmentation with learned programmatic atomic facts (structured textual statements) and a recursive lookahead search. Our agent learns to extract task-critical \\"atomic facts\\" from its interaction trajectories. These facts, serving as a form of programmatic representation, dynamically augment the prompts provided to LLM-based components responsible for action proposal, latent world model simulation (akin to using programs as environment models), and state-value estimation. Planning is performed via a depth-limited lookahead search, where the LLM simulates potential trajectories and evaluates their outcomes, guided by these accumulated programmatic facts and interaction history. This approach allows the agent to improve its understanding and decision-making online. By representing experience as a set of programmatic facts, the agent refines its behavior in-context, enhancing interpretability and efficiency without weight updates. We provide a theoretical motivation linking performance to the quality of fact-based abstraction and LLM simulation accuracy. Empirically, our agent demonstrates improved performance and adaptability on challenging interactive tasks, achieving more optimal behavior as it accumulates experience-grounded programmatic knowledge, showcased in tasks such as TextFrozenLake and ALFWorld."},"pdf":{"value":"/pdf/f906b7b6a790eb76ba547e95b0904ffa156fb7c9.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"supplementary_material":{"value":"/attachment/146390f015de1350fe6c0ef29e4b02ccc459510b.zip"},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Samuel_Holt1"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025leveraging,\\ntitle={Leveraging Learned Programmatic Facts for Enhanced {LLM} Agent Planning and World Modeling},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=nqWsNxbkDV}\\n}"},"paperhash":{"value":"holt|leveraging_learned_programmatic_facts_for_enhanced_llm_agent_planning_and_world_modeling","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission31/Authors"]}},"number":31,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1748689584271,"cdate":1748689584271,"tmdate":1749884845026,"mdate":1749884845026,"pdate":1749884844999,"version":2,"details":{"writable":true,"replyCount":4,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Long Paper (up to 9 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"CMdtl83aZF","forum":"CMdtl83aZF","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission30/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission30/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission30/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission30/Authors"],"content":{"title":{"value":"FormulaCode: Evaluating Agentic Superoptimization on Large Codebases"},"authors":{"value":["Atharva Sehgal","James Hou","Swarat Chaudhuri","Jennifer J. Sun","Yisong Yue"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission30/Authors"]},"authorids":{"value":["~Atharva_Sehgal1","~James_Hou1","~Swarat_Chaudhuri2","~Jennifer_J._Sun1","~Yisong_Yue1"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission30/Authors"]},"keywords":{"value":["Superoptimization","Benchmarking Agents","Evoluationary Algorithms","Repository-level code synthesis"]},"TLDR":{"value":"FormulaCode is a continuously updating benchmark that complements SWE-Bench for evaluating optimization agents (like AlphaEvolve)"},"track":{"value":"Long Paper (up to 9 pages)"},"abstract":{"value":"Rapid advances in LLM agents have shown the ability to optimize code using continuous objective functions — a significant leap beyond traditional code generation techniques. However, there is an urgent need for novel benchmarks that can effectively measure this capability and translate it into real-world impact. Current code benchmarks, which often rely on binary pass/fail outcomes, offer a limited evaluation framework that falls short of capturing the full potential of these emerging capabilities.  To bridge this gap, we introduce FormulaCode, a novel benchmark designed for evaluating agentic superoptimization on large codebases, with a focus on real-world performance optimization.  Constructed from a dataset of 451 real-world performance bottlenecks automatically mined from Github, FormulaCode enables comprehensive testing of an agent's ability to triage, diagnose, and resolve inefficiencies in realistic software environments. FormulaCode proves to be a challenging benchmark for frontier LLMs and agentic frameworks, with unrestricted repository exploration emerging as a principal component for finding performance inefficiencies. By introducing FormulaCode, our goal is to drive the development of next‑generation optimization algorithms that meet the rigorous demands of real‑world software projects."},"pdf":{"value":"/pdf/f461aa49fdd04055284c7579d2297c6eaaf07973.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"supplementary_material":{"value":"/attachment/d4720e6046648ae69b42dee3570921ed14932593.pdf"},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~James_Hou1"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025formulacode,\\ntitle={FormulaCode: Evaluating Agentic Superoptimization on Large Codebases},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=CMdtl83aZF}\\n}"},"paperhash":{"value":"sehgal|formulacode_evaluating_agentic_superoptimization_on_large_codebases","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission30/Authors"]}},"number":30,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1748686213184,"cdate":1748686213184,"tmdate":1749884844944,"mdate":1749884844944,"pdate":1749884844904,"version":2,"details":{"writable":true,"replyCount":3,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Long Paper (up to 9 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"c3PxSjfUYZ","forum":"c3PxSjfUYZ","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission27/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission27/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission27/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission27/Authors"],"content":{"title":{"value":"PDL: Declarative Representation of Agentic Prompting Patterns"},"authors":{"value":["Mandana Vaziri","Louis Mandel","Martin Hirzel","Anca Sailer","Yuji Watanabe","Hirokuni Kitahara"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission27/Authors"]},"authorids":{"value":["~Mandana_Vaziri1","~Louis_Mandel1","~Martin_Hirzel1","~Anca_Sailer1","~Yuji_Watanabe1","~Hirokuni_Kitahara1"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission27/Authors"]},"keywords":{"value":["Prompt Programming","Agents"]},"TLDR":{"value":"We present PDL, a declarative representation for agentic prompting patterns, which facilitates customization by bringing prompts to the forefront and supporting the composition of LLM calls together with code at a high-level of abstraction."},"track":{"value":"Short Paper (up to 4 pages)"},"abstract":{"value":"Prompt engineering for LLMs remains complex, with existing frameworks either hiding complexity behind restrictive APIs or providing inflexible canned patterns that resist customization —making sophisticated agentic programming challenging. We present the Prompt Declaration Language~(PDL), a novel approach to prompt representation that tackles this fundamental complexity by bringing prompts to the forefront, enabling manual and automatic prompt tuning while capturing the composition of LLM calls together with rule-based code and external tools. By abstracting away the plumbing necessary for such compositions, PDL aims at improving programmer productivity while providing a declarative representation that is amenable to optimization. Through a real-world case study, we demonstrate PDL's expressivity and practical utility for implementing complex agentic prompting patterns, showing substantial performance improvement (4x) over conventional approaches."},"pdf":{"value":"/pdf/04fb56ef159be21aaa2e8af027127f2377b651fa.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"supplementary_material":{"value":"/attachment/41774054203c8ef1d981e9360357163a7c71febb.zip"},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Mandana_Vaziri1"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025pdl,\\ntitle={{PDL}: Declarative Representation of Agentic Prompting Patterns},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=c3PxSjfUYZ}\\n}"},"paperhash":{"value":"vaziri|pdl_declarative_representation_of_agentic_prompting_patterns","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission27/Authors"]}},"number":27,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1748661124461,"cdate":1748661124461,"tmdate":1749884844751,"mdate":1749884844751,"pdate":1749884844732,"version":2,"details":{"writable":true,"replyCount":4,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Short Paper (up to 4 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"XDMRtSRnaO","forum":"XDMRtSRnaO","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission26/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission26/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission26/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission26/Authors"],"content":{"title":{"value":"Zero-Shot Instruction Following in RL via Structured LTL Representations"},"authors":{"value":["Mattia Giuri","Mathias Jackermeier","Alessandro Abate"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission26/Authors"]},"authorids":{"value":["~Mattia_Giuri1","~Mathias_Jackermeier1","~Alessandro_Abate1"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission26/Authors"]},"keywords":{"value":["reinforcement learning","instruction following","ltl","generalization"]},"track":{"value":"Long Paper (up to 9 pages)"},"abstract":{"value":"Linear temporal logic (LTL) is a compelling framework for specifying complex, structured tasks for reinforcement learning (RL) agents. Recent work has shown that interpreting LTL instructions as finite automata, which can be seen as high-level programs monitoring task progress, enables learning a single generalist policy capable of executing arbitrary instructions at test time. However, existing approaches fall short in environments where multiple high-level events (i.e., atomic propositions) can be true at the same time and potentially interact in complicated ways. In this work, we propose a novel approach to learning a multi-task policy for following arbitrary LTL instructions that addresses this shortcoming. Our method conditions the policy on sequences of simple Boolean formulae, which directly align with transitions in the automaton, and are encoded via a graph neural network (GNN) to yield structured task representations. Experiments in a complex chess-based environment demonstrate the advantages of our approach."},"pdf":{"value":"/pdf/4d61074a42c1bced935bc60bfcd9e44f7fe69943.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Mattia_Giuri1"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025zeroshot,\\ntitle={Zero-Shot Instruction Following in {RL} via Structured {LTL} Representations},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=XDMRtSRnaO}\\n}"},"paperhash":{"value":"giuri|zeroshot_instruction_following_in_rl_via_structured_ltl_representations","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission26/Authors"]}},"number":26,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1748654644880,"cdate":1748654644880,"tmdate":1749884844744,"mdate":1749884844744,"pdate":1749884844729,"version":2,"details":{"writable":true,"replyCount":4,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Long Paper (up to 9 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"rCUJFqn9YO","forum":"rCUJFqn9YO","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission25/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission25/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission25/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission25/Authors"],"content":{"title":{"value":"EditLord: Learning Code Transformation Rules for Code Editing"},"authors":{"value":["Weichen Li","Albert Jan","Baishakhi Ray","Junfeng Yang","Chengzhi Mao","Kexin Pei"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission25/Authors"]},"authorids":{"value":["~Weichen_Li1","~Albert_Jan1","~Baishakhi_Ray2","~Junfeng_Yang1","~Chengzhi_Mao2","~Kexin_Pei1"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission25/Authors"]},"keywords":{"value":["Large Language Models","Machine Learning for Code Editing","Code Generation","Reasoning"]},"TLDR":{"value":"The paper introduces EditLord, a novel framework that enhances LLMs for code editing by introducing discrete and explicit procedural steps, leading to improved performance on various code editing tasks and better robustness."},"track":{"value":"Long Paper (up to 9 pages)"},"abstract":{"value":"Code editing is a foundational task in software development, where its effectiveness depends on whether it introduces desired code property changes without changing the original code's intended functionality. \\nExisting approaches often formulate code editing as an implicit end-to-end task, omitting the fact that code-editing procedures inherently consist of discrete and explicit steps, and thus suffer from suboptimal performance and lack of robustness and generalization. \\nWe introduce EditLord, a code editing framework that makes the code transformation steps explicit. \\nOur key insight is to employ a language model (LM) as an inductive learner to extract code editing rules from the training code pairs as concise meta-rule sets.\\nSuch rule sets will be manifested for each training sample to augment them for finetuning or assist in prompting- and iterative-based code editing.\\nEditLord outperforms the state-of-the-art by an average of 22.7% in editing performance and 58.1% in robustness while achieving 20.2% higher functional correctness, across critical software engineering and security applications, LM models, and editing modes."},"pdf":{"value":"/pdf/8fa39b61fdf3a54be98d422c823ba5dcfc56c3fc.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Weichen_Li1"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025editlord,\\ntitle={EditLord: Learning Code Transformation Rules for Code Editing},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=rCUJFqn9YO}\\n}"},"paperhash":{"value":"li|editlord_learning_code_transformation_rules_for_code_editing","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission25/Authors"]}},"number":25,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1748593033284,"cdate":1748593033284,"tmdate":1749884844712,"mdate":1749884844712,"pdate":1749884844699,"version":2,"details":{"writable":true,"replyCount":4,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Long Paper (up to 9 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"T7tfgX37ql","forum":"T7tfgX37ql","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission24/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission24/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission24/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission24/Authors"],"content":{"title":{"value":"Time to Impeach LLM-as-a-Judge: Programs are the Future of Evaluation"},"authors":{"value":["Tzu-Heng Huang","Harit Vishwakarma","Frederic Sala"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission24/Authors"]},"authorids":{"value":["~Tzu-Heng_Huang1","~Harit_Vishwakarma1","~Frederic_Sala1"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission24/Authors"]},"keywords":{"value":["Program Synthesis","Automated Assessment","Weak Supervision","Reward Model","LLM-as-judge"]},"TLDR":{"value":"We propose a low-cost, bias-reduced evaluation system to assess LLM through aggregated synthesized judging programs."},"track":{"value":"Short Paper (up to 4 pages)"},"abstract":{"value":"Large language models (LLMs) are widely used to evaluate the quality of LLM generations and responses, but this leads to significant challenges: high API costs, uncertain reliability, inflexible pipelines, and inherent biases. To address these, we introduce **PAJAMA** (Program-As-a-Judge Automated Model Assessment), a new alternative that uses LLMs to *synthesize executable judging programs* instead of directly scoring responses. These synthesized programs can be stored and run locally, costing orders of magnitude less while providing interpretable, and auditable judging logic that can be easily adapted. Program-based judges mitigate biases, improving  judgment consistency by **15.83%** and reducing biased responses  by **23.7%** on average compared to a Qwen2.5-14B-based LLM-as-a-judge. When program judgments are distilled into a  model, PAJAMA outperforms LLM-as-a-judge on the challenging CHAT-HARD subset of RewardBench, outperforming metrics by **2.19%** on Prometheus and **8.67%** on JudgeLM dataset, all at three orders of magnitude lower cost."},"pdf":{"value":"/pdf/a5236c2683286acc7d4ec18a687ebe730bc7c2b2.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Tzu-Heng_Huang1"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"supplementary_material":{"value":"/attachment/9ec2b6852901027c97e8eb6b8b34ba89726d0761.zip"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025time,\\ntitle={Time to Impeach {LLM}-as-a-Judge: Programs are the Future of Evaluation},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=T7tfgX37ql}\\n}"},"paperhash":{"value":"huang|time_to_impeach_llmasajudge_programs_are_the_future_of_evaluation","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission24/Authors"]}},"number":24,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1748591064280,"cdate":1748591064280,"tmdate":1749884844575,"mdate":1749884844575,"pdate":1749884844556,"version":2,"details":{"writable":true,"replyCount":3,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Short Paper (up to 4 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"wr47LsSUjH","forum":"wr47LsSUjH","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission23/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission23/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission23/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission23/Authors"],"content":{"title":{"value":"InstructFlow: Adaptive Symbolic Constraint-Guided Code Generation for Long-Horizon Planning"},"authors":{"value":["Haotian Chi","Zeyu Feng","Yueming Lyu","Chengqi Zheng","Linbo Luo","Yew-Soon Ong","Ivor Tsang","Hechang Chen","Yi Chang","Haiyan Yin"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission23/Authors"]},"authorids":{"value":["~Haotian_Chi2","~Zeyu_Feng1","~Yueming_Lyu1","~Chengqi_Zheng1","~Linbo_Luo1","~Yew-Soon_Ong1","~Ivor_Tsang1","~Hechang_Chen2","~Yi_Chang4","~Haiyan_Yin1"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission23/Authors"]},"keywords":{"value":["Code generation; Task planning; Large language models"]},"track":{"value":"Long Paper (up to 9 pages)"},"abstract":{"value":"Long-horizon planning in robotic manipulation tasks requires translating underspecified, symbolic goals into executable control programs satisfying spatial, temporal, and physical constraints. However, language model-based planners often struggle with long-horizon task decomposition, robust constraint satisfaction, and adaptive failure recovery. We introduce InstructFlow, a multi-agent framework that establishes a symbolic, feedback-driven flow of information for code generation in robotic manipulation tasks. InstructFlow employs a InstructFlow Planner to construct and traverse a hierarchical instruction graph that decomposes goals into semantically meaningful subtasks, while a Code Generator generates executable code snippets conditioned on this graph. Crucially, when execution failures occur, a Constraint Generator analyzes feedback and induces symbolic constraints, which are propagated back into the instruction graph to guide targeted code refinement without regenerating from scratch. This dynamic, graph-guided flow enables structured, interpretable, and failure-resilient planning, significantly improving task success rates and robustness across diverse manipulation benchmarks, especially in constraint-sensitive and long-horizon scenarios."},"pdf":{"value":"/pdf/68f596877ee234802d2853f31eac4e0fe258cdc3.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Haotian_Chi2"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"supplementary_material":{"value":"/attachment/6e0f00b84a05df83de4c6993fc77fe0099d552a5.zip"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025instructflow,\\ntitle={InstructFlow: Adaptive Symbolic Constraint-Guided Code Generation for Long-Horizon Planning},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=wr47LsSUjH}\\n}"},"paperhash":{"value":"chi|instructflow_adaptive_symbolic_constraintguided_code_generation_for_longhorizon_planning","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission23/Authors"]}},"number":23,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1748575300076,"cdate":1748575300076,"tmdate":1749884844579,"mdate":1749884844579,"pdate":1749884844537,"version":2,"details":{"writable":true,"replyCount":4,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Long Paper (up to 9 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"pTOQwAQ14h","forum":"pTOQwAQ14h","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission22/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission22/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission22/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission22/Authors"],"content":{"title":{"value":"Sketch-Plan-Generalize : Learning and Planning with Neuro-Symbolic Programmatic Representations for Inductive Spatial Concepts"},"authors":{"value":["Namasivayam Kalithasan","Sachit Sachdeva","Himanshu Gaurav Singh","Vishal Bindal","Arnav Tuli","Gurarmaan Singh Panjeta","Harsh Himanshu Vora","Divyanshu Agarwal","Rohan Paul","Parag Singla"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission22/Authors"]},"authorids":{"value":["~Namasivayam_Kalithasan1","~Sachit_Sachdeva1","~Himanshu_Gaurav_Singh1","~Vishal_Bindal2","~Arnav_Tuli1","~Gurarmaan_Singh_Panjeta1","cs1210548@cse.iitd.ac.in","~Divyanshu_Agarwal1","~Rohan_Paul1","~Parag_Singla1"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission22/Authors"]},"keywords":{"value":["Concept Learning","Neuro-Symbolic Planning","Continual Learning","Learning from Demonstrations"]},"track":{"value":"Long Paper (up to 9 pages)"},"abstract":{"value":"Effective human-robot collaboration requires the ability to learn personalized concepts from a limited number of demonstrations, while exhibiting inductive generalization, hierarchical composition, and adaptability to novel constraints. Existing approaches that use code generation capabilities of pre-trained large (vision) language models as well as purely neural models show poor generalization to _a-priori_ unseen complex concepts.  Neuro-symbolic methods offer a promising alternative by searching in program space, but face challenges in large program spaces due to the inability to effectively guide the search using demonstrations. Our key insight is to factor inductive concept learning as: (i) _Sketch:_ detecting and inferring a coarse signature of a new concept (ii) _Plan:_ performing an MCTS search over grounded action sequences guided by human demonstrations (iii) _Generalize:_ abstracting out  grounded plans as inductive programs. Our pipeline facilitates generalization and modular re-use, enabling continual concept learning.  Our approach combines the benefits of code generation ability of large language models (LLMs) along with grounded neural representations, resulting in neuro-symbolic programs that show stronger inductive generalization on the task of constructing complex structures vis-'a-vis LLM-only and purely neural approaches. Further, we demonstrate reasoning and planning capabilities with learned concepts for embodied instruction following."},"pdf":{"value":"/pdf/a211ec99823891197cecc31ada8863031e79849a.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Namasivayam_Kalithasan1"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025sketchplangeneralize,\\ntitle={Sketch-Plan-Generalize : Learning and Planning with Neuro-Symbolic Programmatic Representations for Inductive Spatial Concepts},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=pTOQwAQ14h}\\n}"},"paperhash":{"value":"kalithasan|sketchplangeneralize_learning_and_planning_with_neurosymbolic_programmatic_representations_for_inductive_spatial_concepts","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission22/Authors"]}},"number":22,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1748538837923,"cdate":1748538837923,"tmdate":1749884844529,"mdate":1749884844529,"pdate":1749884844488,"version":2,"details":{"writable":true,"replyCount":3,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Long Paper (up to 9 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"mXDB3wuXhN","forum":"mXDB3wuXhN","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission21/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission21/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission21/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission21/Authors"],"content":{"title":{"value":"Discovering Logic-Informed Intrinsic Rewards to Explain Human Policies"},"authors":{"value":["Chengzhi Cao","Yinghao Fu","Chao Yang","Shuang Li"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission21/Authors"]},"authorids":{"value":["~Chengzhi_Cao1","~Yinghao_Fu1","~Chao_Yang9","~Shuang_Li3"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission21/Authors"]},"keywords":{"value":["Inverse Reinforcement Learning","Neural Logic Tree","Reward Learning"]},"track":{"value":"Long Paper (up to 9 pages)"},"abstract":{"value":"In high-stakes fields like healthcare, it is crucial to distill valuable strategic insights from expert clinicians. This paper focuses on extracting these knowledge-based insights from demonstrations provided by experts, where we represent the knowledge as a set of logical rules. Our learning framework is built upon the classic  Inverse Reinforcement Learning (IRL). We assume that experts, like clinicians, are rational, and the treatments they choose are the best choices based on their logical understanding of the situation. Our algorithm can automatically extract these logical rules from their demonstrations. We introduce a neural logic tree generator, which is trained to generate logical statements step by step, starting from the goal and working backward. This mirrors the way humans engage in backward reasoning. Similarly, we interpret policy planning as a forward reasoning process, where the optimal policy is determined by finding the best path forward based on the provided rules. The neural logic tree generator and the policy are learned using the IRL until convergence. This process ultimately leads to the discovery of the most effective strategic rules. As a bonus, our algorithm also allows us to recover the reward function. In our experiments, we demonstrate that our method excels at discovering meaningful logical rules, particularly in the context of healthcare."},"pdf":{"value":"/pdf/009cc3a9debbb4252239a4b35beb83cc2b9c2f4b.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Chengzhi_Cao1"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025discovering,\\ntitle={Discovering Logic-Informed Intrinsic Rewards to Explain Human Policies},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=mXDB3wuXhN}\\n}"},"paperhash":{"value":"cao|discovering_logicinformed_intrinsic_rewards_to_explain_human_policies","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission21/Authors"]}},"number":21,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1748526227254,"cdate":1748526227254,"tmdate":1749884844385,"mdate":1749884844385,"pdate":1749884844335,"version":2,"details":{"writable":true,"replyCount":4,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Long Paper (up to 9 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"Va937YZ9it","forum":"Va937YZ9it","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission20/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission20/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission20/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission20/Authors"],"content":{"title":{"value":"Searching Latent Program Spaces"},"authors":{"value":["Matthew Macfarlane","Clément Bonnet"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission20/Authors"]},"authorids":{"value":["~Matthew_Macfarlane1","~Clément_Bonnet1"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission20/Authors"]},"keywords":{"value":["Test-Time Compute","Latent Program Search","Deep Learning","Meta-Learning"]},"TLDR":{"value":"We introduce the Latent Program Network (LPN), a new neural architecture that builds test-time search directly inside its inference, by learning and searching through a latent space of implicit programs before generating outputs."},"track":{"value":"Long Paper (up to 9 pages)"},"abstract":{"value":"General intelligence requires systems that acquire new skills efficiently and generalize beyond their training distributions. Although program synthesis approaches have strong generalization power, they face scaling issues due to large combinatorial spaces that quickly make them impractical and require human-generated DSLs or pre-trained priors to narrow this search space. On the other hand, deep learning methods have had high successes, but they lack structured test-time adaptation and rely on heavy stochastic sampling or expensive gradient updates for fine-tuning. In this work, we propose the Latent Program Network (LPN), a new architecture that builds in test-time search directly into neural models. LPN learns a latent space of implicit programs---neurally mapping inputs to outputs---through which it can search using gradients at test time. LPN combines the adaptability of symbolic approaches and the scalability of neural methods. It searches through a compact latent space at test time and bypasses the need for pre-defined domain-specific languages. On a range of programming-by-examples tasks, LPN either outperforms or matches performance compared to in-context learning and test-time training methods. Tested on the ARC-AGI benchmark, we demonstrate that LPN can both learn a compact program space and search through it at test time to adapt to novel tasks. LPN doubles its performance on out-of-distribution tasks when test-time search is switched on."},"pdf":{"value":"/pdf/2f2abc35483f42c316d90a6f44e407d60c26dc99.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"supplementary_material":{"value":"/attachment/56f5e2f36e9ec33710a15fc59882f97bf5dd41f0.zip"},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Matthew_Macfarlane1"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025searching,\\ntitle={Searching Latent Program Spaces},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=Va937YZ9it}\\n}"},"paperhash":{"value":"macfarlane|searching_latent_program_spaces","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission20/Authors"]}},"number":20,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1748477335501,"cdate":1748477335501,"tmdate":1749884844379,"mdate":1749884844379,"pdate":1749884844333,"version":2,"details":{"writable":true,"replyCount":3,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Long Paper (up to 9 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"5zwkRf8Etp","forum":"5zwkRf8Etp","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission19/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission19/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission19/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission19/Authors"],"content":{"title":{"value":"Lifelong Experience Abstraction and Planning"},"authors":{"value":["Peiqi Liu","Jiayuan Mao","Leslie Pack Kaelbling","Joshua B. Tenenbaum"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission19/Authors"]},"authorids":{"value":["~Peiqi_Liu2","~Jiayuan_Mao1","~Leslie_Pack_Kaelbling1","~Joshua_B._Tenenbaum1"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission19/Authors"]},"keywords":{"value":["Continual Learning","Human-in-the-Loop","Long-Horizon Tasks Planning","Embodied Planning","Large Language Models","Benchmarking"]},"TLDR":{"value":"A framework for continual behavior learning in embodied agents through interaction with the environment and guidance from humans"},"track":{"value":"Long Paper (up to 9 pages)"},"abstract":{"value":"We present LEAP (Lifelong Experience Abstraction and Planning), a framework for continual behavior learning in embodied agents through interaction with the environment and guidance from humans. LEAP addresses the challenge of representing flexible knowledge about tasks and environments --- ranging from constraints and subgoal sequences to action plans and high-level goals --- in a unified framework. At its core, LEAP builds on the Crow Definition Language (CDL), a behavior rule language that integrates imperative programming with declarative planning by allowing agents to express both executable subroutines and subgoal hierarchies. Leveraging large language models (LLMs), LEAP translates diverse human instructions into CDL programs, generates planning-compatible code, and abstracts reusable behavior rules from successful executions to support future generalization. LEAP maintains a library of such CDL programs, enabling the agent to accumulate and refine its behavioral repertoire over time. We evaluate LEAP on the VirtualHome benchmark, demonstrating its ability to represent a wide variety of human instructions and its capacity to continually improve task performance through experience and interaction."},"pdf":{"value":"/pdf/7efaff07f15490fb85cee18fe9fa632becf331e3.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Peiqi_Liu2"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025lifelong,\\ntitle={Lifelong Experience Abstraction and Planning},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=5zwkRf8Etp}\\n}"},"paperhash":{"value":"liu|lifelong_experience_abstraction_and_planning","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission19/Authors"]}},"number":19,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1748438035999,"cdate":1748438035999,"tmdate":1749884844271,"mdate":1749884844271,"pdate":1749884844242,"version":2,"details":{"writable":true,"replyCount":4,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Long Paper (up to 9 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"RTI8LsGjSi","forum":"RTI8LsGjSi","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission18/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission18/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission18/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission18/Authors"],"content":{"title":{"value":"Afterburner: Reinforcement Learning Facilitates Self-Improving Code Efficiency Optimization"},"authors":{"value":["Mingzhe Du","Anh Tuan Luu","Yue Liu","Yuhao QING","Dong HUANG","Xinyi He","Qian Liu","Zejun MA","See-Kiong Ng"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission18/Authors"]},"authorids":{"value":["~Mingzhe_Du1","~Anh_Tuan_Luu2","~Yue_Liu10","~Yuhao_QING1","~Dong_HUANG4","~Xinyi_He2","~Qian_Liu2","~Zejun_MA1","~See-Kiong_Ng1"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission18/Authors"]},"keywords":{"value":["Code Generation"]},"TLDR":{"value":"This paper introduces an iterative optimization framework that leverages real-time execution feedback to improve code efficiency."},"track":{"value":"Long Paper (up to 9 pages)"},"abstract":{"value":"Large Language Models (LLMs) generate functionally correct solutions but often fall short in code efficiency, a critical bottleneck for real-world deployment. In this paper, we introduce a novel test-time iterative optimization framework to address this, employing a closed-loop system where LLMs iteratively refine code based on empirical performance feedback from an execution sandbox. We explore three training strategies: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Group Relative Policy Optimization~(GRPO). Experiments on our Venus dataset and the APPS benchmark show that SFT and DPO rapidly saturate in efficiency gains. In contrast, GRPO, using reinforcement learning (RL) with execution feedback, continuously optimizes code performance, significantly boosting both pass@1 (from 47% to 62%) and the likelihood of outperforming human submissions in efficiency (from 31% to 45%). Our work demonstrates effective test-time code efficiency improvement and critically reveals the power of RL in teaching LLMs to truly self-improve code efficiency."},"pdf":{"value":"/pdf/efc6f85c270b546a434dfa415f54b4d07895b7db.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Mingzhe_Du1"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025afterburner,\\ntitle={Afterburner: Reinforcement Learning Facilitates Self-Improving Code Efficiency Optimization},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=RTI8LsGjSi}\\n}"},"paperhash":{"value":"du|afterburner_reinforcement_learning_facilitates_selfimproving_code_efficiency_optimization","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission18/Authors"]}},"number":18,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1748434869415,"cdate":1748434869415,"tmdate":1749884844148,"mdate":1749884844148,"pdate":1749884844125,"version":2,"details":{"writable":true,"replyCount":4,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Long Paper (up to 9 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"QznWjA6XTK","forum":"QznWjA6XTK","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission17/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission17/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission17/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission17/Authors"],"content":{"title":{"value":"How Robust Reinforcement Learning Enables Courier-Friendly Route Planning for Last-Mile Delivery?"},"authors":{"value":["Ziying Jia","Zeyu Dong","Miao Yin","Sihong He"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission17/Authors"]},"authorids":{"value":["~Ziying_Jia1","~Zeyu_Dong3","~Miao_Yin1","~Sihong_He1"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission17/Authors"]},"keywords":{"value":["Reinforcement Learning","Last-Mile Delivery","Robust Optimization","Human-centric Design"]},"TLDR":{"value":"This paper proposes a courier-friendly Robust RL-based Smooth and Stable Routing Algorithm for dynamic last-mile delivery."},"track":{"value":"Long Paper (up to 9 pages)"},"abstract":{"value":"Last-mile delivery (LMD) systems increasingly face dynamic customer demands that introduce uncertainty and lead to unstable delivery routes, reducing efficiency and placing cognitive burdens on couriers. To address this, we propose R$^3$S$^2$Route, a Robust Regularizer-enhanced RL-based Smooth and Stable Routing Algorithm that learns courier-friendly policies under state uncertainty. Our method adopts an actor-critic reinforcement learning framework and incorporates a robustness regularizer to penalize policy sensitivity to input perturbations. We formally define route smoothness and stability as courier-friendliness metrics, and integrate them into the learning framework to produce routing policies that are both geometrically intuitive and keep spatial-temporal consistent. Experimental results demonstrate that R$^3$S$^2$Route achieves up to 59.68\\\\% improvement in route smoothness and 14.29\\\\% in route stability, while maintaining low travel distances and time-window violation rates, outperforming several baselines in dynamic delivery environments."},"pdf":{"value":"/pdf/25ce9cf487f69ce12cd17192c02fc1be709fccea.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Ziying_Jia1"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025how,\\ntitle={How Robust Reinforcement Learning Enables Courier-Friendly Route Planning for Last-Mile Delivery?},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=QznWjA6XTK}\\n}"},"paperhash":{"value":"jia|how_robust_reinforcement_learning_enables_courierfriendly_route_planning_for_lastmile_delivery","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission17/Authors"]}},"number":17,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1748420941094,"cdate":1748420941094,"tmdate":1749884844012,"mdate":1749884844012,"pdate":1749884843994,"version":2,"details":{"writable":true,"replyCount":4,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Long Paper (up to 9 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"TrFxW944EE","forum":"TrFxW944EE","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission16/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission16/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission16/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission16/Authors"],"content":{"title":{"value":"Interpretable Reward Modeling with Active Concept Bottlenecks"},"authors":{"value":["Sonia Laguna","Kasia Kobalczyk","Julia E Vogt","Mihaela van der Schaar"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission16/Authors"]},"authorids":{"value":["~Sonia_Laguna1","~Kasia_Kobalczyk1","~Julia_E_Vogt1","~Mihaela_van_der_Schaar2"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission16/Authors"]},"keywords":{"value":["Reward Modeling","Concept Bottleneck Models","Active Learning","Interpretability in RLHF","Acquisition Functions"]},"TLDR":{"value":"This paper presents CB-RM, an interpretable reward modeling framework that uses concept bottlenecks and active learning with Expected Information Gain to improve sample efficiency and transparency in RLHF settings."},"track":{"value":"Short Paper (up to 4 pages)"},"abstract":{"value":"We introduce Concept Bottleneck Reward Models (CB-RM), a reward modeling framework that enables interpretable preference learning through selective concept annotation. Unlike standard RLHF methods that rely on opaque reward functions, CB-RM decomposes reward prediction into human-interpretable concepts. To make this framework efficient in low-supervision settings, we formalize an active learning strategy that dynamically acquires the most informative concept labels. We propose an acquisition function based on Expected Information Gain and show that it significantly accelerates concept learning without compromising preference accuracy. Evaluated on UltraFeedback, our method outperforms baselines in interpretability and sample efficiency, marking a step toward more transparent, auditable, and human-aligned reward models."},"pdf":{"value":"/pdf/d418a337e01f91ca88005a9c36e352f3ef2b4fce.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Kasia_Kobalczyk1"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025interpretable,\\ntitle={Interpretable Reward Modeling with Active Concept Bottlenecks},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=TrFxW944EE}\\n}"},"paperhash":{"value":"laguna|interpretable_reward_modeling_with_active_concept_bottlenecks","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission16/Authors"]}},"number":16,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1748417546051,"cdate":1748417546051,"tmdate":1749884844008,"mdate":1749884844008,"pdate":1749884843991,"version":2,"details":{"writable":true,"replyCount":4,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Short Paper (up to 4 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"xO4HjrkF2r","forum":"xO4HjrkF2r","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission15/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission15/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission15/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission15/Authors"],"content":{"title":{"value":"Weak-for-Strong: Training Weak Meta-Agent to Harness Strong Executors"},"authors":{"value":["Fan Nie","Lan Feng","Haotian Ye","Weixin Liang","Pan Lu","Huaxiu Yao","Alexandre Alahi","James Zou"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission15/Authors"]},"authorids":{"value":["~Fan_Nie1","~Lan_Feng1","~Haotian_Ye1","~Weixin_Liang1","~Pan_Lu2","~Huaxiu_Yao1","~Alexandre_Alahi3","~James_Zou1"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission15/Authors"]},"keywords":{"value":["large language models","reinforcement learning","workflow generation"]},"TLDR":{"value":"We represent agentic workflow as code and train a weak meta-agent to better leverage strong models according to environment feedback through reinforcement learning."},"track":{"value":"Long Paper (up to 9 pages)"},"abstract":{"value":"Efficiently leveraging of the capabilities of contemporary large language models (LLMs) is increasingly challenging, particularly when direct fine-tuning is expensive and often impractical. Existing training-free methods, including manually or automated designed workflows, typically demand substantial human effort or yield suboptimal results. This paper proposes Weak-for-Strong Harnessing (W4S), a novel framework that customizes smaller, cost-efficient language models to design and optimize workflows for harnessing stronger models. W4S formulates workflow design as a multi-turn markov decision process and introduces reinforcement learning for agentic workflow optimization (RLAO) to train a weak meta-agent. Through iterative interaction with the environment, the meta-agent learns to design increasingly effective workflows without manual intervention. Empirical results demonstrate the superiority of W4S that our 7B meta-agent, trained with just one GPU hour, outperforms the strongest baseline by 2.9% ～ 24.6% across eleven benchmarks, successfully elevating the performance of state-of-the-art models such as GPT-3.5-Turbo and GPT-4o. Notably, W4S exhibits strong generalization capabilities across both seen and unseen tasks, offering an efficient, high-performing alternative to directly fine-tuning strong models."},"pdf":{"value":"/pdf/44113af22deeb6d634ab6104f313a4c618699ff2.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Fan_Nie1"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025weakforstrong,\\ntitle={Weak-for-Strong: Training Weak Meta-Agent to Harness Strong Executors},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=xO4HjrkF2r}\\n}"},"paperhash":{"value":"nie|weakforstrong_training_weak_metaagent_to_harness_strong_executors","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission15/Authors"]}},"number":15,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1748248522561,"cdate":1748248522561,"tmdate":1749884843973,"mdate":1749884843973,"pdate":1749884843960,"version":2,"details":{"writable":true,"replyCount":3,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Long Paper (up to 9 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"lHssSZqF67","forum":"lHssSZqF67","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission14/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission14/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission14/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission14/Authors"],"content":{"title":{"value":"Leveraging LLM-based sentiment analysis for portfolio optimization with proximal policy optimization"},"authors":{"value":["Kemal Kirtac","Guido Germano"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission14/Authors"]},"authorids":{"value":["~Kemal_Kirtac1","~Guido_Germano1"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission14/Authors"]},"keywords":{"value":["reinforcement learning","proximal policy optimization","stock portfolio optimization","sentiment analysis"]},"track":{"value":"Long Paper (up to 9 pages)"},"abstract":{"value":"Reinforcement learning (RL) offers adaptive solutions to portfolio optimization, yet standard methods such as proximal policy optimization (PPO) rely exclusively on historical price data and overlook the impact of investor sentiment. We introduce sentiment-augmented PPO (SAPPO), a reinforcement learning framework that incorporates real-time sentiment signals extracted from Refinitiv financial news. Daily sentiment scores are generated using LLaMA 3.3. SAPPO integrates these signals into the PPO advantage function via a sentiment-weighted term, enabling allocation strategies that respond to both price movements and market sentiment. Experiments on a three-asset portfolio demonstrate that SAPPO increases the Sharpe ratio from 1.55 to 1.90 and reduces drawdowns relative to PPO. The optimal configuration uses a sentiment influence parameter $\\\\lambda = 0.1$, as validated through ablation studies and statistically significant $t$-tests ($p < 0.001$). These findings show that sentiment-aware reinforcement learning improves trading performance and offers a robust alternative to purely price-based strategies."},"pdf":{"value":"/pdf/77e79a3884a7188a3bd4ef9332c246b5060ba9c0.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Kemal_Kirtac1"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025leveraging,\\ntitle={Leveraging {LLM}-based sentiment analysis for portfolio optimization with proximal policy optimization},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=lHssSZqF67}\\n}"},"paperhash":{"value":"kirtac|leveraging_llmbased_sentiment_analysis_for_portfolio_optimization_with_proximal_policy_optimization","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission14/Authors"]}},"number":14,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1748190912388,"cdate":1748190912388,"tmdate":1749884843884,"mdate":1749884843884,"pdate":1749884843864,"version":2,"details":{"writable":true,"replyCount":3,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Long Paper (up to 9 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"ZM65X3NoTd","forum":"ZM65X3NoTd","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission12/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission12/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission12/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission12/Authors"],"content":{"title":{"value":"Learning Game-Playing Agents with Generative Code Optimization"},"authors":{"value":["Zhiyi Kuang","Ryan Rong","YuCheng Yuan","Allen Nie"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission12/Authors"]},"authorids":{"value":["~Zhiyi_Kuang1","~Ryan_Rong1","~YuCheng_Yuan1","~Allen_Nie1"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission12/Authors"]},"keywords":{"value":["Agent Learning","Large Language Model Optimizer"]},"TLDR":{"value":"We train Atari game-playing agents by optimizing Python programs with LLMs, enabling efficient, self-improving policies that match deep RL performances with far less training time and fewer environment interactions."},"track":{"value":"Short Paper (up to 4 pages)"},"abstract":{"value":"We present a generative optimization approach for learning game-playing agents, where policies are represented as Python programs and refined using large language models (LLMs). Our method treats decision-making policies as self-evolving code, with current observation as input and an in-game action as output, enabling agents to self-improve through execution traces and natural language feedback with minimal human intervention. Applied to Atari games, our game-playing Python program achieves performance competitive with deep reinforcement learning (RL) baselines while using significantly less training time and much fewer environment interactions. This work highlights the promise of programmatic policy representations for building efficient, adaptable agents capable of complex, long-horizon reasoning."},"pdf":{"value":"/pdf/bcca0b40fc13b11844455d93b96288f46bee38e2.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Zhiyi_Kuang1"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025learning,\\ntitle={Learning Game-Playing Agents with Generative Code Optimization},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=ZM65X3NoTd}\\n}"},"paperhash":{"value":"kuang|learning_gameplaying_agents_with_generative_code_optimization","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission12/Authors"]}},"number":12,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1748149726820,"cdate":1748149726820,"tmdate":1749884843778,"mdate":1749884843778,"pdate":1749884843759,"version":2,"details":{"writable":true,"replyCount":4,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Short Paper (up to 4 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"pFyBdPyOCQ","forum":"pFyBdPyOCQ","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission11/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission11/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission11/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission11/Authors"],"content":{"title":{"value":"Making LLMs Program Interpreters via Execution Trace Chain of Thought"},"authors":{"value":["Koshi Eguchi","Takuya Akiba"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission11/Authors"]},"authorids":{"value":["~Koshi_Eguchi1","~Takuya_Akiba2"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission11/Authors"]},"keywords":{"value":["large language models","chain of thought","code execution"]},"TLDR":{"value":"We introduce ET-CoT, an approach where LLMs are fine-tuned on systematic program execution traces to learn to predict code outcomes by generating these traces as a chain of thought."},"track":{"value":"Short Paper (up to 4 pages)"},"abstract":{"value":"Programmatic representations constitute policies, reward functions, environment models, and skill libraries for autonomous agents. However, their practical value hinges on large language models (LLMs) that can understand and reason about code, not merely generate it. A crucial aspect of this reasoning is the ability of LLMs to predict the outcome of the code (or \`\`execute'' it), a critical yet less developed area. Improving this capability is essential for verifiable policies, self-auditing reward functions, and debuggable environment models within program-centric agents.\\n\\nTo address this, we propose \\\\emph{ET-CoT (Execution Trace Chain of Thought)}, an approach where LLMs learn to generate a detailed and systematic program execution trace as a chain of thought to predict program outcomes. Taking Python as an example, we designed a program-execution trace format inspired by recent theoretical advances. Next, we developed a new Python interpreter called \\\\emph{PyTracify}, which outputs these traces during execution. We then generated a large number of traces and fine-tuned an LLM using them. This ET-CoT approach allows the LLMs to execute Python programs consistently by generating the trace as a CoT.  Specifically, our fine-tuned model outperforms other models of comparable size on code execution benchmarks such as CRUXEval-O and LiveCodeBench."},"pdf":{"value":"/pdf/3983c46de009454a23136cb33e26d4602d37a9e0.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Takuya_Akiba2"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025making,\\ntitle={Making {LLM}s Program Interpreters via Execution Trace Chain of Thought},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=pFyBdPyOCQ}\\n}"},"paperhash":{"value":"eguchi|making_llms_program_interpreters_via_execution_trace_chain_of_thought","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission11/Authors"]}},"number":11,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1748139614187,"cdate":1748139614187,"tmdate":1749884843776,"mdate":1749884843776,"pdate":1749884843757,"version":2,"details":{"writable":true,"replyCount":4,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Short Paper (up to 4 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"a0H2x7YeHH","forum":"a0H2x7YeHH","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission10/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission10/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission10/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission10/Authors"],"content":{"title":{"value":"Scalable Gameplay AI through Composition of LLM-Generated Heuristics"},"authors":{"value":["Danrui Li","Sen Zhang","Mubbasir Kapadia"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission10/Authors"]},"authorids":{"value":["~Danrui_Li1","~Sen_Zhang5","~Mubbasir_Kapadia2"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission10/Authors"]},"keywords":{"value":["Gameplay AI","Large Language Models","Program Synthesis","Program-as-Policy","Heuristic Search"]},"TLDR":{"value":"This research proposes having LLMs generate diverse code snippets (heuristics) that are then combined to create an efficient and interpretable game-playing policy, outperforming other LLM-based agents and enabling large-scale testing."},"track":{"value":"Short Paper (up to 4 pages)"},"abstract":{"value":"Prototyping is a critical stage in game development, often aided by gameplay AI that simulates player behavior to support early design evaluation. Recent work has explored the use of Large Language Models (LLMs) as flexible and interpretable gameplay agents, but their high per-decision inference costs hinder scalability. We propose a program-as-policy framework that prompts an LLM to generate a diverse set of heuristic functions. These functions undergo an LLM-free selection and aggregation process to form a composite policy, eliminating the need for costly runtime inference. Applied to strategy-heavy games, our method outperforms recent LLM-based agents in both effectiveness and efficiency, enabling scalable and interpretable game prototyping."},"pdf":{"value":"/pdf/d1953fb13cbf56555fe72f8b7181da471e0284d4.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Danrui_Li1"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025scalable,\\ntitle={Scalable Gameplay {AI} through Composition of {LLM}-Generated Heuristics},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=a0H2x7YeHH}\\n}"},"paperhash":{"value":"li|scalable_gameplay_ai_through_composition_of_llmgenerated_heuristics","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission10/Authors"]}},"number":10,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1748138676114,"cdate":1748138676114,"tmdate":1749884843741,"mdate":1749884843741,"pdate":1749884843725,"version":2,"details":{"writable":true,"replyCount":3,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Short Paper (up to 4 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"zwEUO0KT8G","forum":"zwEUO0KT8G","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission9/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission9/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission9/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission9/Authors"],"content":{"title":{"value":"Learning to Discover Abstractions for LLM Reasoning"},"authors":{"value":["Yuxiao Qu","Anikait Singh","Yoonho Lee","Amrith Setlur","Ruslan Salakhutdinov","Chelsea Finn","Aviral Kumar"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission9/Authors"]},"authorids":{"value":["~Yuxiao_Qu1","~Anikait_Singh1","~Yoonho_Lee1","~Amrith_Setlur1","~Ruslan_Salakhutdinov1","~Chelsea_Finn1","~Aviral_Kumar2"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission9/Authors"]},"keywords":{"value":["Reasoning abstractions; LLM; RL; Structured exploration; Reasoning"]},"TLDR":{"value":"A two-agent training framework for generating and applying reasoning abstractions to solve complex problems."},"track":{"value":"Long Paper (up to 9 pages)"},"abstract":{"value":"Effective reasoning often requires going beyond pattern matching or memorization of solutions to identify and implement ''algorithmic procedures'' that can be used to deduce answers to hard problems. These algorithmic procedures consist of reusable primitives, intermediate results, or procedures that themselves can be applied across many problems. While current methods of RL post-training on long chains of thought ultimately desire to uncover this kind of algorithmic behavior, their sensitivity to benchmarks and the brittle and locally optimal nature of strategies learned by these systems suggest that this is far from a fulfilled promise. To instantiate this, we introduce reasoning abstractions: concise natural language descriptions of procedural and factual knowledge that guide the model toward successful reasoning strategies. We train models to be capable of proposing several useful abstractions given a problem, followed by RL training that incentivizes building a solution while using the information provided by these abstractions. This results in a two-agent cooperative RL training paradigm, RL through Abstraction Discovery (RLAD), that jointly trains an abstraction generator and an abstraction-conditioned solution generator. This bi-level setup effectively enables structured exploration, decouples learning signals pertaining to abstraction proposal and solution generation, and improves generalization to harder problems, analogous to what we would expect from hierarchical RL. Empirically, RLAD improves performance on challenging math benchmarks."},"pdf":{"value":"/pdf/990edb57f289d485faf04a38b616e0e03c62af83.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Yuxiao_Qu1"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025learning,\\ntitle={Learning to Discover Abstractions for {LLM} Reasoning},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=zwEUO0KT8G}\\n}"},"paperhash":{"value":"qu|learning_to_discover_abstractions_for_llm_reasoning","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission9/Authors"]}},"number":9,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1748127439794,"cdate":1748127439794,"tmdate":1749884843601,"mdate":1749884843601,"pdate":1749884843559,"version":2,"details":{"writable":true,"replyCount":4,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Long Paper (up to 9 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"rpjWkcL7eS","forum":"rpjWkcL7eS","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission8/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission8/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission8/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission8/Authors"],"content":{"title":{"value":"Learned Representations Enhance Multi Agent Path Planning"},"authors":{"value":["Marius Captari","Herke van Hoof"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission8/Authors"]},"authorids":{"value":["~Marius_Captari1","~Herke_van_Hoof4"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission8/Authors"]},"keywords":{"value":["Multi-Agent Pathfinding","Planning and machine learning"]},"TLDR":{"value":"We introduce a method that learns to adjust MAPF edge costs—via black-box gradients—to guide fast heuristic planners toward near-optimal, collision-free multi-agent paths with minimal overhead."},"track":{"value":"Short Paper (up to 4 pages)"},"abstract":{"value":"Multi-Agent Pathfinding (MAPF) involves coordinating multiple agents to find collision-free paths in a shared environment. For large-scale instances, sub-optimal heuristics can be used that are either hand-crafted or learned from data. In this paper, we attempt to combine these approaches by training a neural network to modify problem representations such that Prioritized Planning, a conventional heuristic solver, will produce closer-to-optimal solutions. Thereby, we can leverage the strong performance of existing heuristics with the flexibility of data-driven algorithms. Training the neural network requires propagating learning signals through prioritized planning. This is achieved by calculating gradients of a relaxation of the algorithm using a black-box differentiation approach. Experiments on standard MAPF benchmarks demonstrate that our approach reduces PP's optimality gap without significantly compromising computational efficiency."},"pdf":{"value":"/pdf/f4975e2f0ec8dfba0c9c8290b13d878fe7665417.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Marius_Captari1"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025learned,\\ntitle={Learned Representations Enhance Multi Agent Path Planning},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=rpjWkcL7eS}\\n}"},"paperhash":{"value":"captari|learned_representations_enhance_multi_agent_path_planning","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission8/Authors"]}},"number":8,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1748099504150,"cdate":1748099504150,"tmdate":1749884843597,"mdate":1749884843597,"pdate":1749884843559,"version":2,"details":{"writable":true,"replyCount":4,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Short Paper (up to 4 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"OWDBiMKYdo","forum":"OWDBiMKYdo","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission7/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission7/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission7/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission7/Authors"],"content":{"title":{"value":"DyPO: Dynamic Policy Optimization for Multi-Turn Interactive Reasoning"},"authors":{"value":["Xiao Feng","Bo Han","Zhanke Zhou","Jiaqi Fan","Jiangchao Yao","Ka Ho Li","Dahai Yu","Michael Ng"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission7/Authors"]},"authorids":{"value":["~Xiao_Feng2","~Bo_Han1","~Zhanke_Zhou1","~Jiaqi_Fan1","~Jiangchao_Yao1","~Ka_Ho_Li1","~Dahai_Yu1","~Michael_Ng4"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission7/Authors"]},"keywords":{"value":["Large Language Models; Interactive Reasoning"]},"TLDR":{"value":"We propose a reinforcement learning post-training approach that enhances the multi-turn reasoning capabilities of large language models in dynamic environments."},"track":{"value":"Long Paper (up to 9 pages)"},"abstract":{"value":"Existing on-policy reinforcement learning methods, such as Group Relative Policy Optimization (GRPO) and its variants, have enhanced the reasoning capabilities of large language models (LLMs). However, these methods often rely on static, pre-trained knowledge to navigate partially observed contexts, limiting their effectiveness in dynamic and evolving environments. In such settings, LLMs must actively interact with the environment to gather critical information, necessitating further advancements in adaptive reasoning strategies.\\n    To mitigate this gap, we introduce $\\\\textbf{Dy}$namic $\\\\textbf{P}$olicy $\\\\textbf{O}$ptimization (DyPO), which extends GRPO for multi-turn optimization in dynamic environments. \\n    In principle, DyPO guarantees the  shifting of reasoning pattern from static to dynamic multi-turn reasoning and stablize the training process involving environmental information. \\n    DyPO incorporates four key innovations: \\n    (1) distinct thinking and action tokens that integrate real-time environmental feedback during rollouts, (2) removal of divergence regularization for dynamic reasoning transition,\\n    (3) masked intermediate observations with simplified advantage estimation for enhanced stability, and (4) auxiliary resampling with rejection sampling to mitigate over-generation noise. \\n    These enhancements enable DyPO to achieve adaptive alignment with multi-turn interactive reasoning. Evaluations on challenging simulated benchmarks, ALFWorld and WebShop, using two instantiations of DyPO with Qwen-2.5-3B-Instruct consistently demonstrate substantial improvements in both interactive decision-making and reasoning capabilities compared to existing approaches."},"pdf":{"value":"/pdf/f27c325e6de5aa6eec8abe72ab98f5c6591fb334.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Xiao_Feng2"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025dypo,\\ntitle={Dy{PO}: Dynamic Policy Optimization for Multi-Turn Interactive Reasoning},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=OWDBiMKYdo}\\n}"},"paperhash":{"value":"feng|dypo_dynamic_policy_optimization_for_multiturn_interactive_reasoning","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission7/Authors"]}},"number":7,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1748074538274,"cdate":1748074538274,"tmdate":1749884843528,"mdate":1749884843528,"pdate":1749884843476,"version":2,"details":{"writable":true,"replyCount":4,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Long Paper (up to 9 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"u7a95cXSd4","forum":"u7a95cXSd4","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission6/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission6/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission6/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission6/Authors"],"content":{"title":{"value":"Large Language Models Can Think and Act Probabilistically"},"authors":{"value":["Kou Misaki","Takuya Akiba"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission6/Authors"]},"authorids":{"value":["~Kou_Misaki2","~Takuya_Akiba2"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission6/Authors"]},"keywords":{"value":["Large Language Models"]},"TLDR":{"value":"This research demonstrates that our prompting method can enable agents to reliably execute intended probabilistic behavior."},"track":{"value":"Short Paper (up to 4 pages)"},"abstract":{"value":"This research demonstrates that our non-trivial prompting method, incorporating programmatic representations, can enable agents to reliably execute their own intended probabilistic behavior. This capability is crucial for applications requiring strategic unpredictability (i.e., anti-predictive against adversaries) and efficient exploration. Our proposed prompting method, called Random String Manipulation (RSM), leverages the capability of Large Language Models (LLMs) to generate complex strings and arithmetically manipulate them to select an action from a set of actions according to a given probability distribution. Experiments on tasks requiring probabilistic responses show that RSM consistently outperforms baseline prompts across all tested LLMs, and in some cases achieves performance comparable to pseudo-random number generators, demonstrating its effectiveness in ensuring robust and unbiased probabilistic outputs."},"pdf":{"value":"/pdf/e3b6c21e50923ef8ae0a95dbc1476f205fdc2476.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Kou_Misaki2"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025large,\\ntitle={Large Language Models Can Think and Act Probabilistically},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=u7a95cXSd4}\\n}"},"paperhash":{"value":"misaki|large_language_models_can_think_and_act_probabilistically","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission6/Authors"]}},"number":6,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1747912960198,"cdate":1747912960198,"tmdate":1749884843364,"mdate":1749884843364,"pdate":1749884843346,"version":2,"details":{"writable":true,"replyCount":3,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Short Paper (up to 4 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"fCcN2pJ1Vx","forum":"fCcN2pJ1Vx","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission5/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission5/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission5/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission5/Authors"],"content":{"title":{"value":"ReasonRec: A Reasoning-Augmented Multimodal Agent for Unified Recommendation"},"authors":{"value":["Yihua Zhang","Xi Liu","Xihuan Zeng","Mingfu Liang","Jiyan Yang","Rong Jin","Wen-Yen Chen","Yiping Han","Bo Long","Huayu Li","Buyun Zhang","Liang Luo","Sijia Liu","Tianlong Chen"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission5/Authors"]},"authorids":{"value":["~Yihua_Zhang1","~Xi_Liu1","~Xihuan_Zeng1","~Mingfu_Liang1","~Jiyan_Yang2","~Rong_Jin3","~Wen-Yen_Chen1","~Yiping_Han1","~Bo_Long3","~Huayu_Li3","~Buyun_Zhang1","~Liang_Luo2","~Sijia_Liu1","~Tianlong_Chen1"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission5/Authors"]},"keywords":{"value":["Recommendation Agent","LLM Reasoning"]},"TLDR":{"value":"We introduced a reasoning-augmented multimodal recommendation agent structured around an explicit Observe–Deliberate–Act pipeline."},"track":{"value":"Long Paper (up to 9 pages)"},"abstract":{"value":"Recent advances in multimodal recommenders excel at feature fusion but remain opaque and inefficient decision-makers, lacking explicit reasoning and self-awareness of uncertainty. To address this, we introduce ReasonRec, a reasoning-augmented multimodal agent structured around a three-stage explicit reasoning pipeline: Observe, via a pretrained Vision-Language Model (VLM) encoder; Deliberate, by formulating recommendation as chain-of-thought (CoT) reasoning tasks and explicitly quantifying prediction uncertainty through an evidence-horizon-aware curriculum; and Act, through dynamic delegation of uncertain or challenging queries to lightweight classical recommendation models. Specifically, we propose a reasoning-aware visual instruction tuning strategy that systematically transforms diverse recommendation tasks into unified CoT prompts, enabling the VLM to explicitly articulate intermediate decision steps. Additionally, our evidence-horizon curriculum progressively enhances the reasoning complexity to better handle cold-start and long-tail user scenarios, significantly boosting model generalization. Furthermore, the uncertainty-guided delegation mechanism empowers the agent to assess its own confidence, strategically allocating computational resources to optimize both recommendation accuracy and inference efficiency. Comprehensive experiments on four standard recommendation tasks (sequential recommendation, direct recommendation, CTR prediction, and explanation generation) across five real-world datasets demonstrate that ReasonRec achieves over 30% relative improvement in key ranking metrics (e.g., HR@5, NDCG@5) compared to state-of-the-art multimodal recommenders. Crucially, ReasonRec substantially reduces inference latency by dynamically delegating up to 35% of queries to efficient sub-models without compromising accuracy. Extensive ablation studies further confirm that each proposed reasoning and planning mechanism individually contributes substantially to ReasonRec's overall effectiveness. Collectively, our results illustrate a clear pathway towards interpretable, adaptive, and efficient multimodal recommendation through explicit reasoning and agentic design."},"pdf":{"value":"/pdf/3b6135c8c5ebe9e4cc1bb66c6fcb2fe1434fe5e0.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Yihua_Zhang1"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025reasonrec,\\ntitle={ReasonRec: A Reasoning-Augmented Multimodal Agent for Unified Recommendation},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=fCcN2pJ1Vx}\\n}"},"paperhash":{"value":"zhang|reasonrec_a_reasoningaugmented_multimodal_agent_for_unified_recommendation","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission5/Authors"]}},"number":5,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1747891051711,"cdate":1747891051711,"tmdate":1749884843338,"mdate":1749884843338,"pdate":1749884843193,"version":2,"details":{"writable":true,"replyCount":4,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Long Paper (up to 9 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"cnKYqIgNy5","forum":"cnKYqIgNy5","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission4/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission4/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission4/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission4/Authors"],"content":{"title":{"value":"Inefficiencies of Meta Agents for Agent Design"},"authors":{"value":["Batu El","Mert Yuksekgonul","James Zou"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission4/Authors"]},"authorids":{"value":["~Batu_El1","~Mert_Yuksekgonul1","~James_Zou1"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission4/Authors"]},"keywords":{"value":["LLM/AI agents","Meta Learning"]},"TLDR":{"value":"We identify key challenges in meta-agent-driven agent design: (1) naïve reuse of discovered agents hinders meta-learning, (2) evolutionary methods improve performance but reduce diversity, and (3) design costs are often not justified."},"track":{"value":"Short Paper (up to 4 pages)"},"abstract":{"value":"Recent works began to automate the design of agentic systems using meta-agents that propose and iteratively refine new agent architectures. In this paper, we examine three key challenges in a common class of meta-agents. \\\\emph{First}, we investigate how a meta-agent learns across iterations and find that simply expanding the context with all previous agents, as proposed by previous works, performs worse than ignoring prior designs entirely. We show that the performance improves with an evolutionary approach. \\\\emph{Second}, although the meta-agent designs multiple agents during training, it typically commits to a single agent at test time. We find that the designed agents have low behavioral diversity, limiting the potential for their complementary use. \\\\emph{Third}, we assess when automated design is economically viable. We find that only in a few cases—specifically, two datasets—the overall cost of designing and deploying the agents is lower than that of human-designed agents when deployed on over 15,000 examples. In contrast, the performance gains for other datasets do not justify the design cost, regardless of scale."},"pdf":{"value":"/pdf/c551607af7b91d751029c10d52c0793d6ad259a8.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Batu_El1"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025inefficiencies,\\ntitle={Inefficiencies of Meta Agents for Agent Design},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=cnKYqIgNy5}\\n}"},"paperhash":{"value":"el|inefficiencies_of_meta_agents_for_agent_design","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission4/Authors"]}},"number":4,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1747881393009,"cdate":1747881393009,"tmdate":1749884843221,"mdate":1749884843221,"pdate":1749884843193,"version":2,"details":{"writable":true,"replyCount":4,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Short Paper (up to 4 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}},{"id":"DFplmbCyR4","forum":"DFplmbCyR4","license":"CC BY 4.0","signatures":["ICML.cc/2025/Workshop/PRAL/Submission3/Authors"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission3/Reviewers","ICML.cc/2025/Workshop/PRAL/Submission3/Authors"],"writers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission3/Authors"],"content":{"title":{"value":"Improving Parallel Program Performance with LLM Optimizers via Agent-System Interfaces"},"authors":{"value":["Anjiang Wei","Allen Nie","Thiago S. F. X. Teixeira","Rohan Yadav","Wonchan Lee","Ke Wang","Alex Aiken"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission3/Authors"]},"authorids":{"value":["~Anjiang_Wei1","~Allen_Nie1","~Thiago_S._F._X._Teixeira1","~Rohan_Yadav1","~Wonchan_Lee1","~Ke_Wang1","~Alex_Aiken1"],"readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission3/Authors"]},"keywords":{"value":["large language model","agent","parallel programming","performance optimization"]},"track":{"value":"Long Paper (up to 9 pages)"},"abstract":{"value":"Modern scientific discovery increasingly relies on high-performance computing for complex modeling and simulation. A key challenge in improving parallel program performance is efficiently mapping tasks to processors and data to memory, a process dictated by intricate, low-level system code known as mappers. Developing high-performance mappers demands days of manual tuning, posing a significant barrier for domain scientists without systems expertise. We introduce a framework that automates mapper development with generative optimization, leveraging richer feedback beyond scalar performance metrics. Our approach features the Agent-System Interface, which includes a Domain-Specific Language (DSL) to abstract away low-level complexity of system code and define a structured search space, as well as AutoGuide, a mechanism that interprets raw execution output into actionable feedback. Unlike traditional reinforcement learning methods such as OpenTuner, which rely solely on scalar feedback, our method finds superior mappers in far fewer iterations. With just 10 iterations, it outperforms OpenTuner even after 1000 iterations, achieving $3.8\\\\times$ faster performance. Our approach finds mappers that surpass expert-written mappers by up to $1.34\\\\times$ speedup across nine benchmarks while reducing tuning time from days to minutes."},"pdf":{"value":"/pdf/c8ca5b4c136f37f681a76a423df62bf3238e055c.pdf"},"Format":{"value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track."},"Anonymization":{"value":"This submission has been anonymized for double-blind review via the removal of identifying information such as names, affiliations, and identifying URLs."},"Presenter":{"value":"~Anjiang_Wei1"},"venue":{"value":"ICML 2025 Workshop PRAL"},"venueid":{"value":"ICML.cc/2025/Workshop/PRAL"},"_bibtex":{"value":"@inproceedings{\\nanonymous2025improving,\\ntitle={Improving Parallel Program Performance with {LLM} Optimizers via Agent-System Interfaces},\\nauthor={Anonymous},\\nbooktitle={ICML 2025 Workshop on Programmatic Representations for Agent Learning},\\nyear={2025},\\nurl={https://openreview.net/forum?id=DFplmbCyR4}\\n}"},"paperhash":{"value":"wei|improving_parallel_program_performance_with_llm_optimizers_via_agentsystem_interfaces","readers":["ICML.cc/2025/Workshop/PRAL","ICML.cc/2025/Workshop/PRAL/Submission3/Authors"]}},"number":3,"invitations":["ICML.cc/2025/Workshop/PRAL/-/Submission","ICML.cc/2025/Workshop/PRAL/-/Post_Submission","ICML.cc/2025/Workshop/PRAL/-/Edit"],"domain":"ICML.cc/2025/Workshop/PRAL","tcdate":1747776697807,"cdate":1747776697807,"tmdate":1749884843325,"mdate":1749884843325,"pdate":1749884843182,"version":2,"details":{"writable":true,"replyCount":3,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"track","order":6,"type":"string","input":"radio","value":"Long Paper (up to 9 pages)","description":null},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"Format","order":8,"type":"string","input":"radio","value":"The paper is in either ICML or NeurIPS format and the main paper does not exceed the page limit for our selected track.","description":null},{"name":"supplementary_material","order":9,"type":"file"},{"name":"De-Anonymization","order":10,"type":"string","input":"radio"},{"name":"Presenter","order":11,"type":"group"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"},{"name":"Anonymization"}]}}]`),ka={notes:ya};var Aa=_('<div class="rounded-xl border border-gray-200 p-6 transition hover:border-blue-500/10 hover:shadow-blue-500/10"><h3 class="text-lg font-semibold text-gray-800"><a target="_blank" rel="noreferrer" class="hover:underline"> </a></h3> <p class="mt-2 text-sm text-gray-600"> </p></div>'),Pa=_('<div class="space-y-6"></div>');function Ca(e,a){ge(a,!1);const t=ka.notes;ce();var r=Pa();ta(r,5,()=>t,ea,(n,i)=>{var s=Aa(),h=y(s),d=y(h),k=y(d,!0);L(d),L(h);var o=p(h,2),P=y(o,!0);L(o),L(s),Z(l=>{U(d,"href",`https://openreview.net/forum?id=${J(i).forum}`),V(k,J(i).content.title.value),V(P,l)},[()=>J(i).content.authors.value.join(", ")],ue),R(n,s)}),L(r),R(e,r),fe()}var Ra=_(`<h1 class="mb-6 text-2xl font-bold" id="schedule">Tentative Schedule</h1> <div class="mb-6 text-sm"><p><strong>Location:</strong> West Meeting Room 301-305</p> <p><strong>Room Capacity:</strong> 710</p></div> <div><table class="w-full border-collapse border border-gray-300 text-sm"><thead><tr class="bg-gray-100"><th class="border border-gray-300 px-4 py-2 text-left whitespace-nowrap">Time</th><th class="w-full border border-gray-300 px-4 py-2 text-left">Event</th></tr></thead><tbody><tr class="odd:bg-white even:bg-gray-50"><td class="border border-gray-300 px-4 py-2 whitespace-nowrap">8:20 - 8:30</td><td class="w-full border border-gray-300 px-4 py-2">Opening Remarks</td></tr><tr class="odd:bg-white even:bg-gray-50"><td class="border border-gray-300 px-4 py-2 whitespace-nowrap">8:30 - 9:00</td><td class="w-full border border-gray-300 px-4 py-2">Invited Talk: Animesh Garg</td></tr><tr class="odd:bg-white even:bg-gray-50"><td class="border border-gray-300 px-4 py-2 whitespace-nowrap">9:00 - 9:30</td><td class="w-full border border-gray-300 px-4 py-2">Invited Talk: Amy Zhang</td></tr><tr class="odd:bg-white even:bg-gray-50"><td class="border border-gray-300 px-4 py-2 whitespace-nowrap">9:30 - 10:00</td><td class="w-full border border-gray-300 px-4 py-2">Coffee Break</td></tr><tr class="odd:bg-white even:bg-gray-50"><td class="border border-gray-300 px-4 py-2 whitespace-nowrap">10:00 - 10:15</td><td class="w-full border border-gray-300 px-4 py-2">Oral Presentation: Improving Parallel Program Performance with LLM Optimizers via
							Agent-System Interfaces</td></tr><tr class="odd:bg-white even:bg-gray-50"><td class="border border-gray-300 px-4 py-2 whitespace-nowrap">10:15 - 10:30</td><td class="w-full border border-gray-300 px-4 py-2">Oral Presentation: Searching Latent Program Spaces</td></tr><tr class="odd:bg-white even:bg-gray-50"><td class="border border-gray-300 px-4 py-2 whitespace-nowrap">10:30 - 10:45</td><td class="w-full border border-gray-300 px-4 py-2">Oral Presentation: Lifelong Experience Abstraction and Planning</td></tr><tr class="odd:bg-white even:bg-gray-50"><td class="border border-gray-300 px-4 py-2 whitespace-nowrap">10:45 - 11:00</td><td class="w-full border border-gray-300 px-4 py-2">Sponsor Presentation - <a class="underline" href="https://www.basis.ai/">BASIS</a></td></tr><tr class="odd:bg-white even:bg-gray-50"><td class="border border-gray-300 px-4 py-2 whitespace-nowrap">11:00 - 11:30</td><td class="w-full border border-gray-300 px-4 py-2">Invited Talk: Dale Schuurmans</td></tr><tr class="odd:bg-white even:bg-gray-50"><td class="border border-gray-300 px-4 py-2 whitespace-nowrap">11:30 - 12:00</td><td class="w-full border border-gray-300 px-4 py-2">Invited Talk: Sheila McIlraith</td></tr><tr class="odd:bg-white even:bg-gray-50"><td class="border border-gray-300 px-4 py-2 whitespace-nowrap">12:00 - 13:00</td><td class="w-full border border-gray-300 px-4 py-2">Lunch</td></tr><tr class="odd:bg-white even:bg-gray-50"><td class="border border-gray-300 px-4 py-2 whitespace-nowrap">13:00 - 14:00</td><td class="w-full border border-gray-300 px-4 py-2">Poster Session 1</td></tr><tr class="odd:bg-white even:bg-gray-50"><td class="border border-gray-300 px-4 py-2 whitespace-nowrap">14:00 - 14:30</td><td class="w-full border border-gray-300 px-4 py-2">Invited Talk: Jason Ma</td></tr><tr class="odd:bg-white even:bg-gray-50"><td class="border border-gray-300 px-4 py-2 whitespace-nowrap">14:30 - 15:00</td><td class="w-full border border-gray-300 px-4 py-2">Invited Talk: Wenhao Yu</td></tr><tr class="odd:bg-white even:bg-gray-50"><td class="border border-gray-300 px-4 py-2 whitespace-nowrap">15:00 - 16:00</td><td class="w-full border border-gray-300 px-4 py-2">Poster Session 2</td></tr><tr class="odd:bg-white even:bg-gray-50"><td class="border border-gray-300 px-4 py-2 whitespace-nowrap">16:00 - 16:15</td><td class="w-full border border-gray-300 px-4 py-2">Coffee Break</td></tr><tr class="odd:bg-white even:bg-gray-50"><td class="border border-gray-300 px-4 py-2 whitespace-nowrap">16:15 - 17:00</td><td class="w-full border border-gray-300 px-4 py-2">Panel Discussion</td></tr><tr class="odd:bg-white even:bg-gray-50"><td class="border border-gray-300 px-4 py-2 whitespace-nowrap">17:00 - 17:30</td><td class="w-full border border-gray-300 px-4 py-2">Networking Session</td></tr></tbody></table></div> <div class="mt-4 text-sm text-gray-600 italic"><p>All times are in Pacific Time (PT).</p></div>`,1),Ma=_('<h1 class="mb-4 text-2xl font-bold" id="speakers">Speakers</h1>'),wa=_('<div class="flex flex-col items-center justify-center"><div class="flex flex-wrap justify-center"><!> <!> <!> <!> <!> <!></div></div>'),Ia=_('<h1 class="mb-4 text-2xl font-bold" id="organizers">Organizers</h1>'),_a=_('<div class="flex flex-col items-center justify-center"><div class="flex flex-wrap justify-center"><!> <!> <!> <!> <!> <!> <!> <!> <!></div></div>'),Wa=_('<h1 class="mb-4 text-2xl font-bold" id="call">Call For Papers</h1> <div class="text-base"><!></div>',1),xa=_('<h1 class="mb-4 text-2xl font-bold" id="papers">Accepted Papers</h1> <!>',1),Sa=_(`<div class="text-lg"><div class="flex w-full items-end justify-center font-serif"><div class="mx-8 w-full max-w-2xl pt-8 pb-8 text-black md:pt-28"><div class="mb-4 text-sm"><div class="inline-block rounded-md border border-black px-2 py-1 font-mono">ICML WORKSHOP</div></div> <div class="inline-block text-4xl">Programmatic Representations for Agent Learning</div> <div class="mt-8 flex items-center space-x-2"><div class="font-mono text-sm text-gray-500">Sponsored by</div> <a href="https://basis.ai" target="_blank" rel="noreferrer"><img src="/imgs/basis-logo.svg" alt="Basis Logo" class="h-4"></a></div> <div class="text-md mt-8 text-gray-800">July 18th, 2025</div> <div class="mt-2 text-sm text-gray-500">West Meeting Room 301-305</div> <div class="text-sm text-gray-500">Vancouver, Canada</div></div></div> <div class="sticky top-0 z-10 my-2 flex w-full items-start justify-center bg-gray-100/70 backdrop-blur-md md:py-1.5"><div class="mx-8 w-full max-w-2xl select-none"><div class="my-2 flex flex-row flex-wrap items-center justify-start font-mono text-sm md:text-base"><!> <div class="mx-2 text-gray-400">/</div> <!> <div class="mx-2 text-gray-400">/</div> <!> <div class="mx-2 text-gray-400">/</div> <!> <div class="mx-2 text-gray-400">/</div> <!></div></div></div> <!> <!> <!> <!> <!> <!> <!> <!> <div class="mt-8 flex w-full justify-center bg-gray-100 pt-4 font-sans"><div class="mx-8 w-full max-w-3xl"><div class="flex w-full flex-col items-center justify-start"><div class="mt-4 mb-8 text-2xl text-gray-500"><a href="https://github.com/pr4al-workshop/pr4al-workshop.github.io" target="_blank" rel="noreferrer" class="mr-2"><!></a></div></div> <div class="mb-24 text-sm text-gray-600">This website is licensed under a <a class="underline" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noreferrer">Creative Commons Attribution-ShareAlike 4.0 International License</a> and is based on the <a class="underline" href="https://leela-interp.github.io/" target="_blank" rel="noreferrer">Leela Interp</a> project. That means you're free to borrow the source code of this website with attribution.</div></div></div></div>`);function Fa(e){var a=Sa();Ze(u=>{je.title="Programmatic Representations for Agent Learning Workshop at ICML 2025"});var t=p(y(a),2),r=y(t),n=y(r),i=y(n);q(i,{href:"#schedule",children:(u,W)=>{D();var m=j("SCHEDULE");R(u,m)},$$slots:{default:!0}});var s=p(i,4);q(s,{href:"#speakers",children:(u,W)=>{D();var m=j("SPEAKERS");R(u,m)},$$slots:{default:!0}});var h=p(s,4);q(h,{href:"#organizers",children:(u,W)=>{D();var m=j("ORGANIZERS");R(u,m)},$$slots:{default:!0}});var d=p(h,4);q(d,{href:"#call",children:(u,W)=>{D();var m=j("CALL FOR PAPERS");R(u,m)},$$slots:{default:!0}});var k=p(d,4);q(k,{href:"#papers",children:(u,W)=>{D();var m=j("ACCEPTED PAPERS");R(u,m)},$$slots:{default:!0}}),L(n),L(r),L(t);var o=p(t,2);z(o,{padding:"py-8",class:"md:text-justify",children:(u,W)=>{ba(u)},$$slots:{default:!0}});var P=p(o,2);z(P,{children:(u,W)=>{var m=Ra();D(6),R(u,m)},$$slots:{default:!0}});var l=p(P,2);z(l,{padding:"pb-4",children:(u,W)=>{var m=Ma();R(u,m)},$$slots:{default:!0}});var v=p(l,2);z(v,{children:(u,W)=>{var m=wa(),T=y(m),F=y(T);I(F,{name:"Animesh Garg",affiliation:"Georgia Tech",link:"https://animesh.garg.tech/",image:"https://animesh.garg.tech/assets/animesh-garg-profile-white-nov23.jpg"});var H=p(F,2);I(H,{name:"Amy Zhang",affiliation:"UT Austin",link:"https://amyzhang.github.io/",image:"https://amyzhang.github.io/images/profile.png"});var O=p(H,2);I(O,{name:"Dale Schuurmans",affiliation:"Google DeepMind / University of Alberta",link:"https://webdocs.cs.ualberta.ca/~dale/",image:"https://storage.googleapis.com/gweb-research2023-media/pubtools/4145.png"});var $=p(O,2);I($,{name:"Sheila McIlraith",affiliation:"University of Toronto",link:"https://www.cs.toronto.edu/~sheila/",image:"https://www.cs.toronto.edu/~sheila/img/McIlraith2-lowres.jpg"});var B=p($,2);I(B,{name:"Jason Ma",affiliation:"University of Pennsylvania",link:"https://jasonma2016.github.io/",image:"https://jasonma2016.github.io/jasonma.jpeg"});var X=p(B,2);I(X,{name:"Wenhao Yu",affiliation:"Google DeepMind",link:"https://wenhaoyu.weebly.com/",image:"https://wenhaoyu.weebly.com/uploads/1/4/2/4/14248981/published/img-1723.jpg?1687933779"}),L(T),L(m),R(u,m)},$$slots:{default:!0}});var g=p(v,2);z(g,{padding:"pb-4",children:(u,W)=>{var m=Ia();R(u,m)},$$slots:{default:!0}});var x=p(g,2);z(x,{size:"max-w-4xl",children:(u,W)=>{var m=_a(),T=y(m),F=y(T);I(F,{name:"Shao-Hua Sun",affiliation:"National Taiwan University",link:"https://shaohua0116.github.io/",image:"./imgs/sun.png"});var H=p(F,2);I(H,{name:"Levi Lelis",affiliation:"University of Alberta",link:"https://webdocs.cs.ualberta.ca/~santanad/",image:"https://webdocs.cs.ualberta.ca/~santanad/img/levi_amii.jpg"});var O=p(H,2);I(O,{name:"Xinyun Chen",affiliation:"Google DeepMind",link:"https://jungyhuk.github.io/",image:"https://jungyhuk.github.io/imgs/Xinyun.jpg"});var $=p(O,2);I($,{name:"Shreyas Kapur",affiliation:"UC Berkeley",link:"https://shreyaskapur.com/",image:"./imgs/shreyas.jpg"});var B=p($,2);I(B,{name:"Jiayuan Mao",affiliation:"MIT",link:"https://jiayuanm.com/",image:"https://jiayuanm.com/static/img/avatar24.jpg"});var X=p(B,2);I(X,{name:"Ching-An Cheng",affiliation:"Google Research",link:"https://www.chinganc.com/",image:"https://www.chinganc.com/static/images/ching-an-cheng.jpg"});var ae=p(X,2);I(ae,{name:"Anqi Li",affiliation:"NVIDIA",link:"https://anqili.github.io/",image:"https://anqili.github.io/theme/images/anqi_li.jpg"});var te=p(ae,2);I(te,{name:"Kuang-Huei Lee",affiliation:"Google DeepMind",link:"https://kuanghuei.github.io/",image:"./imgs/kuang.png"});var ke=p(te,2);I(ke,{name:"Leslie Kaelbling",affiliation:"MIT",link:"https://people.csail.mit.edu/lpk/",image:"https://siebelschool.illinois.edu/_sitemanager/viewphoto.aspx?id=10406&s=300"}),L(T),L(m),R(u,m)},$$slots:{default:!0}});var A=p(x,2);z(A,{children:(u,W)=>{var m=Wa(),T=p(oe(m),2),F=y(T);La(F),L(T),R(u,m)},$$slots:{default:!0}});var f=p(A,2);z(f,{children:(u,W)=>{var m=xa(),T=p(oe(m),2);Ca(T,{}),R(u,m)},$$slots:{default:!0}});var c=p(f,2),b=y(c),C=y(b),M=y(C),w=y(M),E=y(w);ma(E,{class:"inline-block hover:text-black"}),L(w),L(M),L(C),D(2),L(b),L(c),L(a),R(e,a)}export{Fa as component};
